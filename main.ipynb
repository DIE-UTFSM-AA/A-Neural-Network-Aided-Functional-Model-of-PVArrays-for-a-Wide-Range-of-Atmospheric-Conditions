{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read xSi12922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xSi12922'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "project_path = os.getcwd()\n",
    "\n",
    "src_path = os.path.join(project_path, 'src')\n",
    "assert os.path.exists(src_path) and os.path.isdir(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l1\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint, TerminateOnNaN, CSVLogger   \n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.dataset import dataset\n",
    "from scipy.constants import Boltzmann as k, eV as q, zero_Celsius as T0\n",
    "from src.PVmodels import Model7PFF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "  def train_step(self, data):\n",
    "    # x: [[S, T], [Rs, Gp, IL, LogI0, b]]\n",
    "    # y: [Isc, Pmp, Imp, Vmp, Voc]\n",
    "    x, y = data\n",
    "    with tf.GradientTape() as tape: \n",
    "      # forward pass\n",
    "      # - compute delta_x\n",
    "      y_pred = self.predict_meas(x, training=True)\n",
    "      \n",
    "      # compute loss\n",
    "      loss = self.compiled_loss(y, y_pred) \n",
    "\n",
    "    # Update weights\n",
    "    grads = tape.gradient(loss, self.trainable_weights)    \n",
    "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    # compute metrics\n",
    "    self.compiled_metrics.update_state(y, y_pred)\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "  def test_step(self, data):\n",
    "    x, y = data\n",
    "    # forward pass\n",
    "    # - compute delta_x\n",
    "    y_pred = self.predict_meas(x, training=False)\n",
    "\n",
    "    # compute loss\n",
    "    # self.compiled_loss(y, y_pred)\n",
    "    loss = self.compiled_loss(y, y_pred) \n",
    "\n",
    "    # compute metrics\n",
    "    self.compiled_metrics.update_state(y, y_pred)\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelDNN(Model7PFF):\n",
    "  def __init__(self, config:dict, project_path:str, seed:int=0):\n",
    "    # check model properties\n",
    "    assert isinstance(config, dict)\n",
    "\n",
    "    assert isinstance(config['PVmodule'], str)\n",
    "    self.PVmodule = config['PVmodule']\n",
    "\n",
    "    assert all([\n",
    "      isinstance(config['ds_ratio'], list),\n",
    "      len(config['ds_ratio']) == 3,  # [train, val, test]\n",
    "    ])\n",
    "\n",
    "    assert isinstance(config['agent'], dict)\n",
    "    self.agent_properties = config['agent']\n",
    "    assert all([\n",
    "      isinstance(self.agent_properties['layers'], list),\n",
    "      len(self.agent_properties['layers']) > 0, # hidden layers\n",
    "    ])\n",
    "    assert all([\n",
    "      isinstance(self.agent_properties['act_hidden'], str),\n",
    "      self.agent_properties['act_hidden'] in ['relu', 'sigmoid', 'tanh']\n",
    "      ])\n",
    "    assert all([\n",
    "      isinstance(self.agent_properties['act_output'], str),\n",
    "      self.agent_properties['act_output'] in ['relu', 'sigmoid', 'tanh', 'linear']\n",
    "      ])\n",
    "    assert all([\n",
    "      isinstance(config['agent']['regularizer'], keras.regularizers.L1) or \n",
    "      isinstance(config['agent']['regularizer'], keras.regularizers.L2),\n",
    "      list(config['agent']['regularizer'].get_config().values())[0] <= 0.3\n",
    "      ])\n",
    "    \n",
    "    # check exists project_path \n",
    "    assert isinstance(project_path, str) \n",
    "    assert self.exists_folder(project_path)\n",
    "    self.path = project_path\n",
    "\n",
    "    # check seed\n",
    "    assert isinstance(seed, int) \n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    # ModelBase config\n",
    "    params = pd.read_csv('data/params.csv', index_col=0).astype(np.float32)\n",
    "    super().__init__(*params.loc[self.PVmodule], 25+T0, 1000)\n",
    "\n",
    "    # dataset config\n",
    "    self.data = dataset(project_path)(self.PVmodule, ratio=config['ds_ratio'], seed=seed)\n",
    "    \n",
    "    # get all (S, T)\n",
    "    allST_physical = copy.deepcopy(self.data['x_data'][0])\n",
    "    allS, allT = np.hsplit(allST_physical, 2)\n",
    "    allT = allT + T0\n",
    "\n",
    "    # calculation of x0 for all examples (in physical units for approach to standardization)\n",
    "    b, IL, I0, Rs, Gp = self.params(allS, allT)\n",
    "    allX0_physical = np.hstack([b, IL, np.log(I0), Rs, Gp])\n",
    "\n",
    "    # measurement in physical units\n",
    "    allY_physical = copy.deepcopy(self.data['y_data'][0])\n",
    "\n",
    "    # Scaler config\n",
    "    self.STscaler = StandardScaler().fit(allST_physical)\n",
    "    self.Xscaler  = StandardScaler().fit(allX0_physical)\n",
    "    self.Yscaler0 = StandardScaler().fit(allY_physical)\n",
    "\n",
    "    # Inverse scaler config\n",
    "    self.InSTscaler = lambda x: x*self.STscaler.var_**.5 + self.STscaler.mean_\n",
    "    self.InXscaler  = lambda x: x* self.Xscaler.var_**.5 +  self.Xscaler.mean_\n",
    "    self.InYscaler  = lambda x: x* self.Yscaler0.var_**.5 +  self.Yscaler0.mean_\n",
    "\n",
    "    # rewrite YScaler (tensorflow)\n",
    "    self.Yscaler = lambda x: (x - self.Yscaler0.mean_)/self.Yscaler0.var_**.5\n",
    "    \n",
    "    # sample standardization\n",
    "    self.ST_tr,  self.Y_tr  = self.standardization(0)\n",
    "    self.ST_val, self.Y_val = self.standardization(1)\n",
    "    self.ST_tst, self.Y_tst = self.standardization(2)\n",
    "    \n",
    "    # Agent congfig\n",
    "    self.agent = self.agent_generation\n",
    "    self.agent.predict_meas = self.y_predict\n",
    "\n",
    "\n",
    "  \n",
    "  def exists_folder(self, dir):\n",
    "    return all([os.path.exists(dir), os.path.isdir(dir)])\n",
    "\n",
    "  def exists_file(self, dir):\n",
    "    return all([os.path.exists(dir), os.path.isfile(dir)])\n",
    "\n",
    "  @property\n",
    "  def agent_generation(self):\n",
    "    # input_layer = keras.Input(shape=[2, ]) # S, T\n",
    "    input_layer = keras.Input(shape=[7, ]) # S, T, x\n",
    "    x = Dense(units=self.agent_properties['layers'][0], \n",
    "              activation=self.agent_properties['act_hidden'], \n",
    "              activity_regularizer=self.agent_properties['regularizer'])(input_layer)\n",
    "    for N_neu in self.agent_properties['layers'][1:]:\n",
    "      x = Dense(units=N_neu, \n",
    "                activation=self.agent_properties['act_hidden'],\n",
    "                activity_regularizer=self.agent_properties['regularizer'])(x)\n",
    "    output_layer = Dense(5, activation=self.agent_properties['act_output'])(x) # delta_x\n",
    "    return CustomModel(input_layer, output_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def standardization(self, idx):\n",
    "    # get measurement (S, T)\n",
    "    st = self.data['x_data'][idx]\n",
    "    st[:,1]+=T0\n",
    "    \n",
    "    # get measurement (Isc, Pmp, Imp, Vmp, Voc)\n",
    "    y = self.data['y_data'][idx]\n",
    "    \n",
    "    return [\n",
    "      self.STscaler.transform(st),\n",
    "      self.Yscaler(y)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  def get_corrected_parameters(self, ST, training=False):\n",
    "    # compute S and T in physical values\n",
    "    ST_phy = self.InSTscaler(ST) # to W/m2 and K\n",
    "\n",
    "    # compute x0 with ModelBase\n",
    "    S, T = np.hsplit(ST_phy, 2) \n",
    "    b, IL, I0, Rs, Gp = self.params(S, T) # [b, IL, I0, Rs, Gp]\n",
    "    x0 = np.hstack([b, IL, np.log(I0), Rs, Gp])\n",
    "\n",
    "    # normalized\n",
    "    x0 = self.Xscaler.transform(x0)\n",
    "    \n",
    "    # compute x_corrected (normalized)\n",
    "    try:\n",
    "      x_corrected = self.agent(ST, training=training) + x0\n",
    "    except:\n",
    "      x_corrected = self.agent(tf.concat([x0, ST], axis=1), training=training) + x0 \n",
    "\n",
    "    # # compute x_corrected (physical)\n",
    "    x_corrected_phy = self.InXscaler(x_corrected)            \n",
    "    \n",
    "    # non-negativity constraints\n",
    "    b, IL, logI0, Rs, Gp = tf.split(x_corrected_phy, axis=1, num_or_size_splits=5)\n",
    "    b, IL, Rs, Gp = [tf.abs(k) for k in [b, IL, Rs, Gp]]\n",
    "    I0 = tf.exp(logI0)\n",
    "    \n",
    "    return [b, IL, I0, Rs, Gp]\n",
    "  \n",
    "  def update_params(self, ST, training=False):\n",
    "    # update function of class 'Model7PFF'\n",
    "    # update params in ModelBase\n",
    "    self.b, self.IL, self.I0, self.Rs, self.Gp = self.get_corrected_parameters(ST, training=training)    \n",
    "\n",
    "  def predict(self, ST, MaxIterations=10000, tol=1e-9, alpha=0.15, beta=0.85, VmpIni=None, training=False): \n",
    "    # update function of class 'Model7PFF'\n",
    "    self.update_params(ST, training=training)\n",
    "    Isc = self.fun_Ipv(0)\n",
    "    Vsc = self.fun_Vpv(Isc)\n",
    "    Voc = self.fun_Vpv(0)\n",
    "    if tf.reduce_any(tf.math.is_nan(Voc)):\n",
    "      Voc = self.funVoc(0)\n",
    "    Ioc = self.fun_Ipv(Voc)   \n",
    "    fun_Vmp = lambda Vmp: self.fun_Ipv(Vmp)-self.fun_foc(Vmp)\n",
    "    IniIterations = 0\n",
    "    if VmpIni==None: \n",
    "      Vmp0, Vmp1 = Voc*alpha, Voc*beta\n",
    "    else: \n",
    "      Vmp0, Vmp1 = VmpIni\n",
    "    error = tf.math.abs(Vmp1-Vmp0)\n",
    "    while tf.reduce_all(tf.math.less_equal(np.float64(tol), error)):\n",
    "      IniIterations+=1\n",
    "      Vmp11 = Vmp0-fun_Vmp(Vmp0)*(Vmp1-Vmp0)/(fun_Vmp(Vmp1)-fun_Vmp(Vmp0))\n",
    "      Vmp0, Vmp1 = Vmp1, Vmp11\n",
    "      error = tf.math.abs(Vmp1-Vmp0)\n",
    "      if IniIterations==MaxIterations:\n",
    "        break\n",
    "    Vmp = Vmp1\n",
    "    Imp = self.fun_Ipv(Vmp)\n",
    "    Pmp = Vmp*Imp\n",
    "    return [Isc, Vsc, Imp, Vmp, Pmp, Ioc, Voc]\n",
    "  \n",
    "  def y_predict(self, ST, training=False):\n",
    "    [Isc, Vsc, Imp, Vmp, Pmp, Ioc, Voc] = self.predict(ST, training=training)\n",
    "    ypred = tf.concat([Isc, Pmp, Imp, Vmp, Voc], axis=1)\n",
    "    return self.Yscaler(ypred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def fit(self, batch_size:int=32, epochs:int=10, patience:int=5, test=False):\n",
    "    \"\"\"\n",
    "      - batch_size: subsample size\n",
    "      - epochs:     training epoch\n",
    "      - patience:   epoch for shutdown criteria\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(batch_size, int)\n",
    "    assert isinstance(epochs, int)\n",
    "    assert isinstance(patience, int)\n",
    "\n",
    "    MSE = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    def metrics(y_true, y_pred, var):\n",
    "      return tf.reduce_mean(tf.math.square(y_true-y_pred), axis=0)[var]\n",
    "      \n",
    "\n",
    "    def Isc(y_true, y_pred):\n",
    "      return metrics(y_true, y_pred, var=0)\n",
    "    \n",
    "    def Pmp(y_true, y_pred):\n",
    "      return metrics(y_true, y_pred, var=1)\n",
    "    \n",
    "    def Imp(y_true, y_pred):\n",
    "      return metrics(y_true, y_pred, var=2)\n",
    "\n",
    "    def Vmp(y_true, y_pred):\n",
    "      return metrics(y_true, y_pred, var=3)\n",
    "    \n",
    "    def Voc(y_true, y_pred):\n",
    "      return metrics(y_true, y_pred, var=4)\n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "      loss = tf.reduce_mean(tf.math.square(y_true-y_pred))\n",
    "      return loss\n",
    "    \n",
    "    self.agent.compile(loss = custom_loss, \n",
    "        metrics=[Isc, Pmp, Imp, Vmp, Voc],\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-6), \n",
    "        run_eagerly=True)\n",
    "    \n",
    "    # return self.agent.fit( self.ST_val[:100], self.Y_val[:100], \n",
    "    #                           batch_size=batch_size, \n",
    "    #                           epochs=epochs,             \n",
    "    #                           callbacks=[History(), TerminateOnNaN()]\n",
    "    #                               )\n",
    "\n",
    "    history = self.agent.fit( \n",
    "                        self.ST_tr, self.Y_tr, \n",
    "                        validation_data=(self.ST_val, self.Y_val),\n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,             \n",
    "                        callbacks=[\n",
    "                                  History(), \n",
    "                                  TerminateOnNaN(),\n",
    "                                  EarlyStopping(\n",
    "                                        patience=patience, \n",
    "                                        monitor=\"val_loss\", \n",
    "                                        restore_best_weights=True),\n",
    "                                  # CSVLogger(\n",
    "                                  #       os.path.join(self.model_seed, 'history.csv'), \n",
    "                                  #       separator=\",\", \n",
    "                                  #       append=False),\n",
    "                                  # ModelCheckpoint(\n",
    "                                  #       filepath=os.path.join(self.model_seed, 'Checkpoint'), \n",
    "                                  #       save_weights_only=True, \n",
    "                                  #       monitor='val_loss', \n",
    "                                  #       mode='min', \n",
    "                                  #       save_best_only=False),\n",
    "                                        ]\n",
    "                                  \n",
    "                            )\n",
    "    return history\n",
    "\n",
    "  def random_search(self):\n",
    "    print('a')\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "  'PVmodule':'xSi12922',\n",
    "  'ds_ratio':[7, 2, 1], \n",
    "  'agent': {\n",
    "    'layers': [100, 70, 40, 10], \n",
    "    'act_hidden': 'relu', \n",
    "    'act_output': 'linear',\n",
    "    'regularizer': l1(0.15),\n",
    "  },\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "dnn = ModelDNN(config, project_path, seed=0)\n",
    "\n",
    "dnn.PVmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "patience = 5 \n",
    "a = \"/home/miguel/A-Neural-Network-Aided-Functional-Model-of-PVArrays-for-a-Wide-Range-of-Atmospheric-Conditions/results\"\n",
    "\n",
    "for k in os.listdir(a):\n",
    "  print('\\n'+k+'\\n')\n",
    "  try:\n",
    "    dnn.agent.load_weights(os.path.join(a, k))\n",
    "    history = dnn.fit(batch_size=batch_size, epochs=epochs, patience=patience, test=True)\n",
    "  except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dadfa6c9e495656b5aeef62e855dddced6b3b09cd40b0d573233c49f51a8556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
